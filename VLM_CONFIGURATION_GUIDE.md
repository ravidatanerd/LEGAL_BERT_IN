# ü§ñ Vision-Language Models (VLM) Configuration Guide

## üìã **What are VLMs?**
Vision-Language Models extract text and understand content from PDF documents without traditional OCR. InLegalDesk supports multiple VLM options with different quality/speed tradeoffs.

---

## üéØ **Quick Setup (Recommended)**

### **üèÜ Best Quality (OpenAI First):**
```bash
# In your .env file:
VLM_ORDER=openai,donut,pix2struct,tesseract_fallback
OPENAI_API_KEY=your_api_key_here
```

### **‚ö° Using Presets (Easiest):**
```bash
# Choose one preset in your .env file:
VLM_PRESET=premium      # Best quality (OpenAI only)
VLM_PRESET=high         # Balanced (OpenAI + local models)
VLM_PRESET=offline      # No API calls needed
VLM_PRESET=fast         # Prioritize speed
```

---

## üîß **Available VLM Models:**

| Model | Quality | Speed | Cost | Requirements |
|-------|---------|-------|------|--------------|
| **openai** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | üí∞üí∞ | OpenAI API Key |
| **donut** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Free | GPU recommended |
| **pix2struct** | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Free | GPU recommended |
| **tesseract_fallback** | ‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Free | Always works |

---

## üéöÔ∏è **Quality Presets Explained:**

### **üèÜ PREMIUM** (`VLM_PRESET=premium`)
- **Models**: OpenAI ‚Üí Tesseract fallback
- **Best for**: Highest accuracy needed
- **Requires**: OpenAI API key
- **Cost**: ~$0.01-0.02 per document page
- **Speed**: Very fast

### **‚öñÔ∏è HIGH** (`VLM_PRESET=high`)
- **Models**: OpenAI ‚Üí Donut ‚Üí Pix2Struct ‚Üí Tesseract
- **Best for**: Production use with fallbacks
- **Requires**: OpenAI API key + local models
- **Cost**: Medium (API + compute)
- **Speed**: Fast

### **‚öñÔ∏è BALANCED** (`VLM_PRESET=balanced`)
- **Models**: Donut ‚Üí OpenAI ‚Üí Pix2Struct ‚Üí Tesseract
- **Best for**: Cost-conscious with API backup
- **Requires**: Local models (GPU recommended)
- **Cost**: Low to medium
- **Speed**: Medium

### **‚ö° FAST** (`VLM_PRESET=fast`)
- **Models**: Tesseract ‚Üí OpenAI
- **Best for**: Speed over accuracy
- **Requires**: Basic OCR + optional API
- **Cost**: Low
- **Speed**: Very fast

### **üîí OFFLINE** (`VLM_PRESET=offline`)
- **Models**: Donut ‚Üí Pix2Struct ‚Üí Tesseract
- **Best for**: No internet/API restrictions
- **Requires**: Local models only
- **Cost**: Free (compute only)
- **Speed**: Slower

### **‚ö° BASIC** (`VLM_PRESET=basic`)
- **Models**: Tesseract only
- **Best for**: Simple text extraction
- **Requires**: Nothing special
- **Cost**: Free
- **Speed**: Very fast

---

## üõ†Ô∏è **Manual Configuration:**

### **Environment Variables:**
```bash
# Option 1: Use preset (easiest)
VLM_PRESET=high

# Option 2: Custom order
VLM_ORDER=openai,donut,pix2struct,tesseract_fallback

# Performance settings
VLM_BATCH_SIZE=4
VLM_TIMEOUT=300
ENABLE_OCR_FALLBACK=true
```

### **API Configuration:**
```bash
# Configure VLM via REST API
POST /vlm/config
{
    "preset": "high"
}

# Or custom order
POST /vlm/config
{
    "custom_order": ["openai", "donut", "tesseract_fallback"]
}

# Get current config
GET /vlm/config

# Get recommendations
GET /vlm/recommendations
```

---

## üéØ **Choosing the Right Configuration:**

### **üíº For Production Legal Research:**
```bash
VLM_PRESET=high
OPENAI_API_KEY=your_key
```
- Best balance of quality and reliability
- OpenAI for complex documents, local models for backup

### **üí∞ For Budget-Conscious Use:**
```bash
VLM_PRESET=offline
```
- No API costs
- Good quality with local models
- Requires GPU for best performance

### **‚ö° For High-Volume Processing:**
```bash
VLM_PRESET=fast
```
- Prioritizes speed
- OCR first, API backup for complex cases

### **üîí For Air-Gapped Environments:**
```bash
VLM_PRESET=offline
```
- No internet required
- All processing local

---

## üîç **Model Processing Order:**

VLMs are tried in order until one succeeds:

1. **First model** attempts extraction
2. If it fails ‚Üí **Second model** tries
3. Continue until success or all models tried
4. **Tesseract fallback** usually works as last resort

### **Example Flow (HIGH preset):**
```
Document ‚Üí OpenAI Vision ‚Üí Success ‚úÖ
                       ‚Üí Fail ‚Üí Donut ‚Üí Success ‚úÖ
                                    ‚Üí Fail ‚Üí Pix2Struct ‚Üí Success ‚úÖ
                                                       ‚Üí Fail ‚Üí Tesseract ‚Üí Success ‚úÖ
```

---

## ‚öôÔ∏è **Advanced Settings:**

### **Performance Tuning:**
```bash
# Batch processing
VLM_BATCH_SIZE=4        # Process 4 pages at once

# Timeout settings
VLM_TIMEOUT=300         # 5 minutes per document

# Memory management
MAX_WORKERS=4           # Parallel processing threads
```

### **Model-Specific Settings:**
```bash
# OpenAI settings
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_MAX_TOKENS=4000

# Local model settings
MODEL_DEVICE=auto       # auto, cpu, cuda
MODEL_CACHE_DIR=./models
```

---

## üö® **Troubleshooting:**

### **OpenAI Model Not Working:**
```bash
# Check API key
echo $OPENAI_API_KEY

# Test API access
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models
```

### **Local Models Not Working:**
```bash
# Check transformers installation
python -c "import transformers; print('OK')"

# Check GPU availability
python -c "import torch; print(torch.cuda.is_available())"
```

### **All Models Failing:**
```bash
# Enable OCR fallback
ENABLE_OCR_FALLBACK=true

# Use basic preset
VLM_PRESET=basic
```

---

## üìä **Performance Comparison:**

| Configuration | Accuracy | Speed | Cost/Doc | GPU Needed |
|---------------|----------|-------|----------|------------|
| **premium** | 95% | 2s | $0.02 | No |
| **high** | 90% | 5s | $0.01 | Optional |
| **balanced** | 85% | 10s | $0.005 | Yes |
| **fast** | 75% | 1s | $0.001 | No |
| **offline** | 80% | 15s | $0 | Yes |
| **basic** | 60% | 0.5s | $0 | No |

---

## üéâ **Getting Started:**

### **1. Quick Start (OpenAI):**
```bash
# Add to .env
VLM_PRESET=premium
OPENAI_API_KEY=your_key_here

# Restart backend
python app.py
```

### **2. No-Cost Setup:**
```bash
# Add to .env
VLM_PRESET=offline

# Install local models
pip install transformers torch

# Restart backend
python app.py
```

### **3. Check Configuration:**
```bash
# Visit in browser
http://localhost:8877/vlm/config

# Or use curl
curl http://localhost:8877/vlm/recommendations
```

**üéä Your VLM configuration is now optimized for your needs!**